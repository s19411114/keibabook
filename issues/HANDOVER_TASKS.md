# ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘ã‘å¼•ç¶™ãã‚¿ã‚¹ã‚¯

**ä½œæˆæ—¥**: 2025å¹´11æœˆ25æ—¥  
**å‚ç…§**: `issues/PERFORMANCE_ANALYSIS.md` ï¼ˆå•é¡Œåˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼‰

---

## ğŸ¯ æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€KeibaBook Scraperã®æ”¹å–„ã‚¿ã‚¹ã‚¯ã®ã†ã¡ã€æ™‚é–“ã®ã‹ã‹ã‚‹ä½œæ¥­ã‚’ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å¼•ãç¶™ããŸã‚ã®æŒ‡ç¤ºæ›¸ã§ã™ã€‚

---

## ã‚¿ã‚¹ã‚¯1: Dockerç’°å¢ƒã®æœ€é©åŒ–

### ç›®çš„
Dockerå†…ã§ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œã‚’é«˜é€ŸåŒ–ã™ã‚‹

### ç¾çŠ¶ã®å•é¡Œ
- `mcr.microsoft.com/playwright/focal` ã‚¤ãƒ¡ãƒ¼ã‚¸ã¯æ¯å› `pip install` ãŒå¿…è¦
- WSL2çµŒç”±ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒã‚¦ãƒ³ãƒˆãŒé…ã„
- Chromiumã®åˆå›èµ·å‹•ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

### ä½œæ¥­å†…å®¹
1. `Dockerfile` ã‚’æ›´æ–°ã—ã¦ä¾å­˜é–¢ä¿‚ã‚’ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
2. `docker-compose.yml` ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
3. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã—ã¦æ™‚é–“ã‚’è¨ˆæ¸¬

### å‚è€ƒã‚³ãƒ¼ãƒ‰

```dockerfile
# Dockerfile æ”¹å–„æ¡ˆ
FROM mcr.microsoft.com/playwright/python:v1.40.0-jammy

WORKDIR /app

# ä¾å­˜é–¢ä¿‚ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦å…ˆã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ï¼‰
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼
COPY . .

CMD ["bash"]
```

```yaml
# docker-compose.yml æ”¹å–„æ¡ˆ
services:
  app:
    build: .
    container_name: keibabook-dev
    volumes:
      # delegated ã§æ›¸ãè¾¼ã¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
      - ${HOST_PROJECT_DIR:-.}:/app:delegated
      - /app/venv
      - /app/__pycache__
      - /app/data  # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã‚‚Dockerãƒœãƒªãƒ¥ãƒ¼ãƒ ã«
    environment:
      - PYTHONUNBUFFERED=1
      - PLAYWRIGHT_BROWSERS_PATH=/ms-playwright
    # ãƒ¡ãƒ¢ãƒªãƒ»CPUåˆ¶é™ã®æ˜ç¤º
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2'
```

### æˆåŠŸåŸºæº–
- Dockerå†…ã§ `run_single_race.py --venue æµ¦å’Œ --race 9` ãŒ60ç§’ä»¥å†…ã«å®Œäº†

---

## ã‚¿ã‚¹ã‚¯2: run_pedigree.py ã®éåŒæœŸåŒ–

### ç›®çš„
è¡€çµ±ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’ä¸¦åˆ—åŒ–ã—ã¦é«˜é€ŸåŒ–

### ç¾çŠ¶ã®å•é¡Œ
- `requests` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§åŒæœŸçš„ã«URLã‚’1ã¤ãšã¤å–å¾—
- æ•°ç™¾URLã®å ´åˆã€æ•°æ™‚é–“ã‹ã‹ã‚‹

### ä½œæ¥­å†…å®¹
1. `requests` ã‚’ `aiohttp` ã«ç½®ãæ›ãˆ
2. `asyncio.Semaphore` ã§ä¸¦åˆ—æ•°ã‚’åˆ¶å¾¡ï¼ˆã‚µã‚¤ãƒˆè² è·è»½æ¸›ï¼‰
3. ã‚¨ãƒ©ãƒ¼å‡¦ç†ã¨ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ

### å‚è€ƒã‚³ãƒ¼ãƒ‰

```python
# run_pedigree_async.pyï¼ˆæ–°è¦ä½œæˆï¼‰
import asyncio
import aiohttp
import json
from pathlib import Path

CONCURRENCY = 3  # åŒæ™‚æ¥ç¶šæ•°
BASE_DELAY = 1.0

async def fetch_pedigree(session, semaphore, url, store_dir):
    async with semaphore:
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=20)) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    # ãƒ‘ãƒ¼ã‚¹å‡¦ç†...
                    return {'url': url, 'status': 'success'}
                elif resp.status == 429:
                    await asyncio.sleep(30)
                    return {'url': url, 'status': 'retry'}
        except Exception as e:
            return {'url': url, 'status': 'error', 'error': str(e)}
        finally:
            await asyncio.sleep(BASE_DELAY)

async def main():
    pq = Path('pedigree_queue.json')
    urls = json.load(open(pq, encoding='utf-8'))
    store_dir = Path('pedigree_store')
    store_dir.mkdir(exist_ok=True)
    
    semaphore = asyncio.Semaphore(CONCURRENCY)
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_pedigree(session, semaphore, url, store_dir) for url in urls]
        results = await asyncio.gather(*tasks)
    
    success = sum(1 for r in results if r['status'] == 'success')
    print(f"Done: {success}/{len(urls)} URLs fetched")

if __name__ == '__main__':
    asyncio.run(main())
```

### æˆåŠŸåŸºæº–
- 100 URLs ãŒ 5åˆ†ä»¥å†…ã«å‡¦ç†å¯èƒ½
- ã‚µã‚¤ãƒˆã‹ã‚‰429ã‚¨ãƒ©ãƒ¼ãŒé »ç™ºã—ãªã„ã“ã¨

---

## ã‚¿ã‚¹ã‚¯3: ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®æ‹¡å……

### ç›®çš„
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®å†ç™ºã‚’é˜²ããŸã‚ã®è‡ªå‹•ãƒ†ã‚¹ãƒˆè¿½åŠ 

### ä½œæ¥­å†…å®¹
1. `tests/test_rate_limiter.py` ã‚’ä½œæˆ
2. `tests/test_scraper_performance.py` ã‚’ä½œæˆ
3. CI/CDã§ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ¤œå‡º

### å‚è€ƒã‚³ãƒ¼ãƒ‰

```python
# tests/test_rate_limiter.py
import pytest
import asyncio
from src.utils.rate_limiter import RateLimiter

@pytest.mark.asyncio
async def test_rate_limiter_max_wait():
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¾…æ©Ÿæ™‚é–“ãŒ3ç§’ã‚’è¶…ãˆãªã„ã“ã¨ã‚’ç¢ºèª"""
    limiter = RateLimiter(base_delay=3.0)
    import time
    start = time.perf_counter()
    await limiter.wait()
    elapsed = time.perf_counter() - start
    assert elapsed < 4.0, f"Wait time {elapsed}s exceeded expected maximum"

@pytest.mark.asyncio
async def test_rate_limiter_custom_delay():
    """ã‚«ã‚¹ã‚¿ãƒ å¾…æ©Ÿæ™‚é–“ãŒæ­£ã—ãé©ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
    limiter = RateLimiter(base_delay=0.5)
    import time
    start = time.perf_counter()
    await limiter.wait(randomize=False)
    elapsed = time.perf_counter() - start
    assert 0.4 < elapsed < 0.7, f"Wait time {elapsed}s not within expected range"
```

```python
# tests/test_scraper_performance.py
import pytest
import asyncio
import time

@pytest.mark.asyncio
@pytest.mark.slow
async def test_single_race_scrape_under_60s():
    """1ãƒ¬ãƒ¼ã‚¹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒ60ç§’ä»¥å†…ã«å®Œäº†ã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
    # ãƒ¢ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    # å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆã§ã¯é©å®œèª¿æ•´
    pass
```

### æˆåŠŸåŸºæº–
- å…¨ãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹
- CIã§5åˆ†ä»¥å†…ã«å…¨ãƒ†ã‚¹ãƒˆå®Œäº†

---

## ã‚¿ã‚¹ã‚¯4: ãƒ­ã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ”¹å–„

### ç›®çš„
é•·æ™‚é–“å®Ÿè¡Œã®åŸå› ã‚’ç‰¹å®šã—ã‚„ã™ãã™ã‚‹

### ä½œæ¥­å†…å®¹
1. å„ãƒ•ã‚§ãƒƒãƒã®æ‰€è¦æ™‚é–“ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆæ—¢å­˜ã® `PERF` ãƒ­ã‚°ã‚’æ´»ç”¨ï¼‰
2. 429ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿå›æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ
3. å®Ÿè¡Œæ™‚é–“ã®ã‚µãƒãƒªãƒ¼ã‚’JSONå‡ºåŠ›

### å‚è€ƒã‚³ãƒ¼ãƒ‰

```python
# src/utils/perf_logger.pyï¼ˆæ–°è¦ä½œæˆï¼‰
import time
import json
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List

@dataclass
class FetchRecord:
    url: str
    status: int
    duration_ms: float
    is_retry: bool = False
    error: str = None

class PerfLogger:
    def __init__(self, race_key: str):
        self.race_key = race_key
        self.records: List[FetchRecord] = []
        self.start_time = time.perf_counter()
    
    def log_fetch(self, url: str, status: int, duration_ms: float, **kwargs):
        self.records.append(FetchRecord(url=url, status=status, duration_ms=duration_ms, **kwargs))
    
    def save_summary(self, output_dir: Path):
        elapsed = time.perf_counter() - self.start_time
        summary = {
            'race_key': self.race_key,
            'total_elapsed_ms': elapsed * 1000,
            'fetch_count': len(self.records),
            'error_count': sum(1 for r in self.records if r.error),
            '429_count': sum(1 for r in self.records if r.status == 429),
            'records': [asdict(r) for r in self.records]
        }
        out_file = output_dir / f'perf_{self.race_key}.json'
        with open(out_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        return out_file
```

### æˆåŠŸåŸºæº–
- å®Ÿè¡Œå¾Œã« `perf_*.json` ãŒç”Ÿæˆã•ã‚Œã‚‹
- 429ã‚¨ãƒ©ãƒ¼å›æ•°ãŒå®¹æ˜“ã«ç¢ºèªã§ãã‚‹

---

## ğŸ“‹ ã‚¿ã‚¹ã‚¯å„ªå…ˆé †ä½

| ã‚¿ã‚¹ã‚¯ | å„ªå…ˆåº¦ | æ¨å®šæ™‚é–“ | æ‹…å½“ |
|--------|--------|----------|------|
| ã‚¿ã‚¹ã‚¯1: Dockeræœ€é©åŒ– | é«˜ | 2-3æ™‚é–“ | æœªå‰²å½“ |
| ã‚¿ã‚¹ã‚¯2: éåŒæœŸåŒ– | ä¸­ | 3-4æ™‚é–“ | æœªå‰²å½“ |
| ã‚¿ã‚¹ã‚¯3: ãƒ†ã‚¹ãƒˆæ‹¡å…… | ä¸­ | 2-3æ™‚é–“ | æœªå‰²å½“ |
| ã‚¿ã‚¹ã‚¯4: ãƒ­ã‚°æ”¹å–„ | ä½ | 1-2æ™‚é–“ | æœªå‰²å½“ |

---

## ğŸ“Œ æ³¨æ„äº‹é …

1. **ã‚µã‚¤ãƒˆè² è·**: ä¸¦åˆ—æ•°ã‚’ä¸Šã’ã™ãã‚‹ã¨429ã‚¨ãƒ©ãƒ¼ãŒå¢—åŠ ã™ã‚‹ãŸã‚ã€CONCURRENCY=3ç¨‹åº¦ã‚’æ¨å¥¨
2. **Cookie**: ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¶­æŒã™ã‚‹ãŸã‚ `cookies.json` ã‚’é©åˆ‡ã«ç®¡ç†
3. **ãƒ†ã‚¹ãƒˆç’°å¢ƒ**: æœ¬ç•ªã‚µã‚¤ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ä¼´ã†ãƒ†ã‚¹ãƒˆã¯ `@pytest.mark.slow` ã§ãƒãƒ¼ã‚¯
