#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾— â†’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ â†’ ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—ã®ä¸€é€£ã®æµã‚Œã‚’ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import sys
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent))

from src.utils.schedule_manager import ScheduleManager
from src.scrapers.race_scraper import RaceScraper
from src.utils.db_manager import CSVDBManager
from src.utils.logger import get_logger

logger = get_logger(__name__)


async def test_full_workflow():
    """
    å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ
    
    1. æœ¬æ—¥ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å–å¾—
    2. ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    3. æœ€åˆã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    4. ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JSONã§ä¿å­˜
    """
    print("=" * 70)
    print("ğŸš€ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾— â†’ DBä¿å­˜ â†’ ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—")
    print("=" * 70)
    
    # 1. ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—
    print("\nğŸ“… STEP 1: æœ¬æ—¥ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—")
    print("-" * 70)
    
    schedule_manager = ScheduleManager()
    schedule = await schedule_manager.get_today_schedule("all")
    
    if not schedule:
        print("âŒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—å¤±æ•—ã€‚é–‹å‚¬ãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return False
    
    print(f"âœ… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—æˆåŠŸ: {len(schedule)} ä¼šå ´")
    for venue_data in schedule:
        venue = venue_data.get('venue', 'ä¸æ˜')
        races = venue_data.get('races', [])
        print(f"  ğŸ‡ {venue}: {len(races)} ãƒ¬ãƒ¼ã‚¹")
    
    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    print("\nğŸ’¾ STEP 2: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜")
    print("-" * 70)
    
    db_manager = CSVDBManager()
    today_str = datetime.now().strftime("%Y-%m-%d")
    db_manager.save_schedule(schedule, today_str)
    
    print(f"âœ… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä¿å­˜å®Œäº†: {today_str}")
    
    # 3. ãƒ¬ãƒ¼ã‚¹IDç”Ÿæˆ
    print("\nğŸ†” STEP 3: ãƒ¬ãƒ¼ã‚¹IDç”Ÿæˆ")
    print("-" * 70)
    
    today_date = datetime.now().date()
    race_ids = schedule_manager.generate_race_ids(schedule, today_date)
    
    if not race_ids:
        print("âŒ ãƒ¬ãƒ¼ã‚¹IDãŒç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False
    
    print(f"âœ… ãƒ¬ãƒ¼ã‚¹IDç”Ÿæˆå®Œäº†: {len(race_ids)} ä»¶")
    print(f"  ã‚µãƒ³ãƒ—ãƒ«: {race_ids[:5]}")
    
    # 4. æœ€åˆã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    print("\nğŸ STEP 4: æœ€åˆã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
    print("-" * 70)
    
    first_race_id = race_ids[0]
    print(f"å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ID: {first_race_id}")
    
    scraper = RaceScraper()
    try:
        race_data = await scraper.scrape_race(first_race_id)
        
        if race_data:
            print(f"\nâœ… ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ!")
            print(f"  ãƒ¬ãƒ¼ã‚¹å: {race_data.get('race_name', 'ä¸æ˜')}")
            print(f"  å‡ºèµ°é¦¬æ•°: {len(race_data.get('horses', []))}")
            print(f"  CPUäºˆæƒ³æ•°: {len(race_data.get('cpu_prediction', []))}")
            print(f"  URL: {race_data.get('url', '')}")
            print(f"  æœ€çµ‚URL: {race_data.get('final_url', '')}")
            
            # CPUäºˆæƒ³ä¸Šä½3é ­ã‚’è¡¨ç¤º
            cpu_pred = race_data.get('cpu_prediction', [])
            if cpu_pred:
                print(f"\n  ğŸ’¡ CPUäºˆæƒ³ TOP 3:")
                for i, pred in enumerate(cpu_pred[:3], 1):
                    print(f"    {i}ä½: {pred.get('horse_name', 'ä¸æ˜')}")
            
            # JSONã§ä¿å­˜
            import json
            output_dir = Path("data/json")
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = output_dir / f"{first_race_id}.json"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(race_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {output_file}")
            
        else:
            print("âŒ ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
            return False
            
    finally:
        await scraper.close()
    
    # 5. ã¾ã¨ã‚
    print("\n" + "=" * 70)
    print("âœ… çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†!")
    print("=" * 70)
    print(f"\nğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
    print(f"  ãƒ»å–å¾—ä¼šå ´æ•°: {len(schedule)}")
    print(f"  ãƒ»ç”Ÿæˆãƒ¬ãƒ¼ã‚¹IDæ•°: {len(race_ids)}")
    print(f"  ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: data/db/schedules.csv")
    print(f"  ãƒ»JSONãƒ•ã‚¡ã‚¤ãƒ«: data/json/{first_race_id}.json")
    print()
    
    return True


if __name__ == '__main__':
    success = asyncio.run(test_full_workflow())
    sys.exit(0 if success else 1)
